1、redis
|-应用场景：
  |--缓存热点数据（修改频率不高，读取频率非常高）
  |--高并发读写
  |--计数器
  |--分布式锁
|-redis是单线程的，不过多个命令就不具备原子性，想要保证原子性，就要通过LUA脚本。
|-RESP 是redis客户端和服务端之前使用的一种通讯协议；实现简单、快速解析、可读性好。
|-批量删除指定库的key：
  |--本地：./redis-cli keys "name*" | xargs ./redis-cli del
  |--远程：./redis-cli -h 10.121.31.83 -p 9002 -n 3 keys "name*" | xargs ./redis-cli -h 10.121.31.83 -p 9002 -n 3 del
|-集群：
  |--redis集群采用P2P模式，是完全去中心化的，不存在中心节点或者代理节点。
  |--Redis集群至少需要3个节点，因为投票容错机制要求超过半数节点认为某个节点挂了该节点才是挂了，所以2个节点无法构成集群。
  |--要保证集群的高可用，需要每个节点都有从节点，也就是备份节点，所以Redis集群至少需要6台服务器。
  |--那么为什么任意一个节点挂了（没有从节点）这个集群就挂了呢？
     |---集群内置了16384个slot（哈希槽）。当需要在Redis集群存放一个数据（key-value）时，redis会先对这个key进行crc16算法，然后得到一个结果。
         再把这个结果对16384进行求余，这个余数会对应[0-16383]其中一个槽，进而决定key-value存储到哪个节点中。
         所以一旦某个节点挂了，该节点对应的slot就无法使用，那么就会导致集群无法正常工作。
|-持久化：
  |--概念：持久化机制把内存中的数据同步到硬盘文件来保证数据持久化。当Redis重启后通过把硬盘文件重新加载到内存，就能达到恢复数据的目的。
  |--RDB：默认
     |---按照一定的时间周期策略把内存的数据以快照的形式保存到硬盘的二进制文件。文件为dump.rdb
     |---功能核心函数rdbSave(生成RDB文件)和rdbLoad（从文件加载内存）两个函数
     |---如：save 900 1【在900秒(15分钟)之后，如果至少有1个key发生变化，则dump内存快照】
     |---性能好【使用单独子进程来进行持久化，主进程不会进行任何IO操作】
     |---恢复速度快
     |---数据容易丢失【RDB是间隔一段时间进行持久化，如果在持久化之前redis宕机】
  |--AOF：
     |---当Redis执行写命令，会通过Write函数追加到redis文件（appendonly.aof）中，类似于MySQL的binlog。
     |---如：appendfsync always/everysec/no【每次有数据修改发生时都会写入AOF文件】
     |---更高的数据完整性
     |---AOF文件较大【】
     |---恢复速度慢【】
|-什么是缓存雪崩？
  |--在同一时刻出现大面积的缓存过期，导致所有原本应该访问缓存的请求都去查询数据库了
  |--解决方法：
     |---1、设置不同的过期时间，可以把过期时间打乱，比如随机获取30分钟内的任意一个值做为过期时间；
     |---2、为了防止真的雪崩，加锁来控制数据库请求
|-什么是缓存穿透？
  |--一般的缓存系统，都是按照key去缓存查询，如果不存在对应的value，就应该去后端系统查找（比如DB）。
     一些恶意的请求会故意查询不存在的key,请求量很大，就会对后端系统造成很大的压力。这就叫做缓存穿透。
  |--解决方法：
     |---简单粗暴：如果一个查询返回的数据为空，我们仍然把这个空结果进行缓存，但它的过期时间会很短，最长不超过五分钟
     |---布隆过滤器（推荐）：将所有可能存在的数据哈希到一个足够大的bitmap中，一个一定不存在的数据会被这个bitmap拦截掉
         |----缺点：可能误判（明明不存在，却判成存在，因为有一个误判率参数）；需要另外维护一个集合来存放缓存的Key；布隆过滤器不支持删值操作
     <dependency>
         <groupId>com.google.guava</groupId>
         <artifactId>guava</artifactId>
         <version>28.0-jre</version>
     </dependency>
     BloomFilter<String> bf = BloomFilter.create(Funnels.stringFunnel(Charset.defaultCharset()), 10000, 0.01);
     bf.put("hello");
     bf.put("world");
     bf.put("java");
     boolean ret = bf.mightContain("java");
     assertTrue(ret);
     boolean ret2 = bf.mightContain("php");
     assertFalse(ret2);
|-什么是缓存击穿？
  |--和缓存雪崩区别在于，针对的是某一个key，在并发访问时，缓存没有，数据库中有，所有的请求都去查询数据库了。
  |--使用互斥锁解决，分布式使用分布式锁
|-单线程的redis为什么这么快
  |--纯内存操作
  |--单线程操作，避免了频繁的上下文切换
  |--采用了非阻塞I/O多路复用机制
|-Memcache和Redis的区别
  |--存储方式：Memecache把数据全部存在内存之中；Redis有部份存在硬盘上，这样能保证数据的持久性。
  |--数据类型：Memcached所有的值均是简单的字符串；Redis支持String、Hash、List、Set、SortedSet
  |--value值大小：Redis最大可以达到1gb；memcache只有1mb
  |--redis的速度比memcached快很多
|-mySQL里有2000w数据，redis中只存20w的数据，如何保证redis中的数据都是热点数据
  |--使用数据淘汰策略（回收策略），6种数据淘汰策略：
     |---volatile-lru：从已设置过期时间的数据集（server.db[i].expires）中挑选最近最少使用的数据淘汰
     |---volatile-ttl：从已设置过期时间的数据集（server.db[i].expires）中挑选将要过期的数据淘汰
     |---volatile-random：从已设置过期时间的数据集（server.db[i].expires）中任意选择数据淘汰
     |---allkeys-lru：从数据集（server.db[i].dict）中挑选最近最少使用的数据淘汰
     |---allkeys-random：从数据集（server.db[i].dict）中任意选择数据淘汰
     |---no-enviction（默认）：禁止驱逐数据
|-Redis 有哪些架构模式？
  |--单机版
  |--主从复制master/slave
     |---1主3从
     |---缺点：当master宕机，需要手动将slave节点提升为master；无法保证高可用，没有解决 master 写的压力
  |--哨兵（Sentinel）
     |---监控（Monitoring）：Sentinel  会不断地检查你的主服务器和从服务器是否运作正常。
     |---提醒（Notification）：当被监控的某个 Redis 服务器出现问题时， Sentinel 可以通过 API 向管理员或者其他应用程序发送通知。
     |---自动故障迁移（Automatic failover）：当哨兵节点都认为主节点故障时，哨兵投票选出的leader进行故障转移，
     |---缺点：需要多部署一套Sentinel集群体系；主从模式，切换需要时间丢数据；没有解决 master 写的压力
  |--集群（直连型）（Redis-Cluster）
     |---无中心架构（不存在哪个节点影响性能瓶颈）。
     |---数据按照 slot 存储分布在多个节点，节点间数据共享，可动态调整数据分布。
     |---可扩展性，可线性扩展到 1000 个节点，节点可动态添加或删除。
     |---实现故障自动转移，用投票机制完成 Slave到 Master 的角色提升
|-节点间的内部通信机制【看图redis集群节点通讯.png】
  |--节点间采取gossip协议进行通信，gossip几种消息类型：Meet、Ping、Pong、Fail等，其中发送Meet消息，用于通知新节点加入
  |--缺点：元数据更新有延时
  |--通讯目的：维护元数据，节点是否出现故障
  |--

2、ElasticSearch
|-版本区别
  |--5.X 版本中，一个 index 下可以创建多个 type
  |--6.X 版本中，一个 index 下只能存在一个 type
  |--7.X 版本中，直接去除了 type 的概念；废除_all
  |--为什么要移除
     1、起初考虑到与关系型数据库一致（index代表库，type代表表）
     2、关系型数据库表是相互独立的；而在Elasticsearch索引里，不同类型的同名字段内部使用的是同一个lucene字段存储，
        也就是type1的userName和type2的userName意义必须相同（如类型一样）。
|-倒排索引：【Elasticsearch分别为每个字段都建立了一个倒排索引】
  |--概念【Term Index：以树的形式缓存在内存中 -> Term Dictionary[Term] -> Posting List：这就是一个倒排索引】
      1、Term（单词）：一段文本经过分析器分析以后就会输出一串单词，这一个一个的就叫做Term
      2、Term Dictionary（单词字典）：顾名思义，它里面维护的是Term，可以理解为Term的集合
      3、Term Index（单词索引）：为了更快的找到某个单词，我们为单词建立索引
      4、Posting List（倒排列表）：倒排列表记录了出现过某个单词的所有文档的文档列表及单词在该文档中出现的位置信息，
         每条记录称为一个倒排项(Posting)。根据倒排列表，即可获知哪些文档包含某个单词。
  |--为什么需要Term Index
     如果直接使用磁盘存储term dictionary，性能差，但整个term dictionary本身又太大，无法完整地放到内存里
|-一个索引相当于mysql的一个数据库：/megacorp/employee/1：megacorp索引名称，employee类型名称，1特定雇员的ID
|-如果持有主分片的节点挂了，副本分片就会晋升为主分片的角色
|-集群
  |--集群最重要的属性：cluster.name
  |--单节点集群的副本分片是没数据的，因为没有意义。
  |--索引创建后就不能改主分片数目，但可以改副本分片数目PUT /blogs/_settings{"number_of_replicas" : 2}
  |--在相同节点数目的集群上增加更多的副本分片并不能提高性能，但是更多的副本分片数提高了数据冗余量，在失去多个节点的情况下不丢失任何数据
  |--如果某个节点挂了，副本分片可以提升为主分片
|-master选举原理（主要是通过ZenDiscovery类来实现的）
  |--Bully算法：对所有候选主节点根据nodeId进行排序
  |--第一种：正常集群（也就是已经选好主节点了），然后加入新节点，会选择接受之前的master， 然后自己连接master并加入这个master构成的集群。
  |--第二种：
     |---Bully算法，每次选举每个节点都把自己所知道节点排一次序，然后选出第一个（第0位）节点，暂且认为它是master节点
     |---如果对某个节点的投票数达到一定的值（可以成为master节点数n/2+1）并且该节点自己也选举自己，那这个节点就是master。否则重新选举
     |---为了防止重新选举主节点时，出现多个主节点，即脑裂现象，可设置最少投票通过数量discovery.zen.minimum_master_nodes=(候选主节点数/2)+1，也就是超过半数
|-节点类型：
  |--候选主节点：
     |---也就是node.mater=true，node.data = false
     |---负责创建索引、删除索引、决定哪些分片分配给相关的节点、追踪集群中的节点状态等工作【注意：分片不会被分配到候选主节点】
  |--数据节点
     |---也就是node.master = false，node.data = true
     |---负责增删改查等操作
     |---默认情况下，node.mater和node.data的值都为true，即该节点既可以做候选主节点也可以做数据节点，
         数据节点承载了数据的操作，负载较重，所以需要考虑将二者分离开，避免因数据节点负载重导致主节点不响应
  |--客户端节点
     |---node.master = false，node.data = false
     |---既不做候选主节点也不做数据节点,只负责请求的分发、汇总等，也就是下面要说到的协调节点的角色，单独增加这样的节点更多是为了负载均衡
  |--协调节点
     |---是一种角色，而不是真实的Elasticsearch的节点，集群中的任何节点，都可以充当协调节点的角色，比如：客户端请求到Node1节点，Node1就是协调节点
|-基础
  |--创建索引
  PUT /log-web
  {
      "settings": {
          "number_of_shards" :   1,
          "number_of_replicas" : 0
      },
      "mappings": {
        "properties": {
          "birth":{
            "type": "date"
          }
        }
      }
  }
  |--查看字段类型：GET /log-web
  |--查询多个文档：
  GET /log-web/_mget
  {
    "ids":["1","3"]
  }
  |--添加文档：POST /log-web/_doc/1/_create【_create是为了防止覆盖原有的文档（_index、_type、_id），如果已存在，会返回error：409】
  |--更新文档：不能被修改，只能被替换，底层会把原来的文档标记为删除，然后添加新文档，后续自动清理已删除的文档
  POST /log-web/_update/6
  {
    "doc": {
      "orgName":"yihang",//如果该字段原本就存在，就会覆盖原来的值，如果不存在，就会给员工1添加该字段。
      "position":"java"
    }
  }
  或者直接添加覆盖
  PUT /company/1
  {
    "empCode":"HNA004",
    "name":"lili",
    "gender":"weman",
    "companyName":"HNA"
  }
  |--删除文档：DELETE /log-web/_doc/1
  或根据条件删除：
  POST /log-web/_delete_by_query
  {
    "query":{
      "term":{
        "_id":"6"
      }
    }
  }
  |--匹配：
  匹配name为“Titanic”的数据
  GET movie/_search
  {
    "query": {
      "match": {
        "name": "Titanic"
      }
    }
  }
  只匹配name="leon"的数据
  GET movie/_search
  {
    "query": {
      "match_phrase": {
        "name": "leon"
      }
    }
  }
  多个字段匹配查询(name和sex都包含女的数据)
  GET people/_search
  {
    "query": {
      "multi_match": {
        "query": "女",
        "fields": ["name","sex"]
      }
    }
  }
  |--模糊查询
  GET /cars/transactions/_search?pretty
  {
    "query": {
      "wildcard": {
        "city": {
          "value": "*ia*"
        }
      }
    }
  }
|-分布式文档存储原理
  |--路由一个文档到一个分片中：shard = hash(routing) % number_of_primary_shards【routing默认是_id】
  |--写文档：
     |---只能在主分片上完成后，再同步到相关的副本分片
     |---客户端向 Node1 发送增删改请求，Node1 成为协调节点
     |---Node1根据文档_id计算出文档在哪个主分片（P1）上，然后将请求转发到 Node2
     |---Node2处理完成后，就开始同步数据到副本分片了，所以又将请求转发到 Node3
     |---Node3处理完成后，响应给 Node2 ，Node2收到响应结果，再响应给 Node1 ，Node1收到响应结果，再响应给客户端
  |--查询一个文档：
     |---查询文档可以从主分片和任意一个副本分片中查询
     |---客户端向 Node1 发送获取请求，Node1 成为协调节点
     |---Node1根据文档_id计算出文档在哪个主分片（P1）上，Node1还会找到P1的所有副本分片（R1），然后通过负载均衡（随机轮询）在P1和R1中随机选择一个
     |---假如选择了P1，P1处理完成后，将结果返回给协调节点，协调节点再返回给客户端
  |--全文搜索文档：
     |---客户端向 Node1 发送获取请求，Node1 成为协调节点
     |---Node1根据倒排索引找到所有符合的文档_id，然后通过这些文档_id循环操作上面的步骤
     |---每个分片将结果返回给协调节点，然后由协调节点进行数据的合并、排序、分页，得到最终的结果，返回给客户端
  |--mget查询多个文档
      1、客户端向 Node 1 发送 mget 请求。
      2、Node 1 为每个分片构建多文档获取请求，然后并行转发这些请求到托管在每个所需的主分片或者副本分片的节点上。
         一旦收到所有答复， Node 1 构建响应并将其返回给客户端。
  |--bulk修改多个文档
      1、客户端向 Node 1 发送 bulk 请求。
      2、Node 1 为每个节点创建一个批量请求，并将这些请求并行转发到每个包含主分片的节点主机。
      3、主分片一个接一个按顺序执行每个操作。当每个操作成功时，主分片并行转发新文档（或删除）到副本分片，然后执行下一个操作。
         一旦所有的副本分片报告所有操作成功，该节点将向协调节点报告成功，协调节点将这些响应收集整理并返回给客户端。
         一旦收到所有答复， Node 1 构建响应并将其返回给客户端。
  |--
|-集群常用命令
  |--http://10.121.31.143:9200/_cat/shards/log-fltnet-web-20191203【使用该命令查看集群下索引的分片信息】
  |--http://10.121.31.143:9200/_cat/allocation?v【查看集群所在磁盘的分配状况】
  |--http://10.121.31.143:9200/_cat/indices?v【查看集群的索引信息】
  |--http://10.121.31.143:9200/_cat/nodes?v【查询节点信息】
  |--http://10.121.31.143:9200/_cluster/health?pretty
     |---green代表所有主分片和副分片都正常运行
     |---yellow：所有主分片可用，但不是所有副本分片都可用，最常见的情景是单节点的时候
     |---red：不是所有的主分片都可用，通常时由于某个索引的住分片为分片unassigned
|-性能优化
  |--节点的平方>=主分片+副分片
  |--索引不要创建太多，尽量不要按天生成
  |--JVM最大内存不要超过32G【JVM在内存小于2GB的时候会采用一个内存对象指针压缩技术，一旦你越过那个神奇的32GB的边界，指针就会切回普通对象的指针。】
  |--尽量让一半以上的数据存储在filesystem cache中，比如：300G的数据，机器的内存需要32G（JVM）+150G（filesystem cache）。
  |--可以做数据预热：写个定时任务，将平时使用得比较多的数据搜索一下，刷到filesystem cache里去
  |--冷（访问很少、频率很低的数据）热数据分离：为了尽量让热数据留在filesystem cache里，别让冷数据给冲刷掉
     假设你有 6 台机器，2 个索引，一个放冷数据，一个放热数据，每个索引 3 个shard。3 台机器放热数据 index；另外 3 台机器放冷数据 index。
  |--document模型设计：不要有类似mysql join的关联查询，最好是先在Java系统里就完成关联，将关联好的数据直接写入es中。
  |--分页性能优化：
     |---举个例子吧，假如你每页是 10 条数据，你现在要查询第 100 页，实际上是会把每个 shard 上存储的前 1000 条数据都查到一个协调节点上，
         如果你有个 5 个shard，那么就有 5000 条数据，接着协调节点对这 5000 条数据进行一些合并、处理，再获取到最终第 100 页的 10 条数据。
         所以用 es 做分页的时候，你会发现越翻到后面，就越是慢
     |---解决方案：使用scroll api，这种做法有一个缺点：不能随意跳到任何一页的场景，只能一页一页的翻
3、RabbitMQ
|-三种队列模式
  |--简单队列：直连模式，1对1
  |--工作模式：生产一条消息，只能一个消费者消费
  |--四种交换机模式
     |---Direct（默认）：在绑定时设定一个routing_key，消息的routing_key匹配时，才会被交换机投送到绑定的队列中去，这种类型干脆就不要用交换机了。
     |---Topic：转发消息主要是根据通配符（*：表示一个词；#：表示零个或多个词）
     |---Fanout：消息广播的模式，routing_key被忽略，给Fanout交换机发送消息，绑定了这个交换机的所有队列都收到这个消息
     |---Headers：不依赖于routing key与binding key的匹配规则来路由消息，而是根据发送的消息内容中的headers属性进行匹配
|-RabbitMQ安装路径：cd /usr/local/Cellar/rabbitmq/3.8.0
|-启动：rabbitmq-server -detached
|-查看状态：rabbitmqctl status
|-关闭：rabbitmqctl stop
|-可视化监控界面：http://localhost:15672【guest/guest】
|-如何确保消息100%发送至RabbitMQ?
    数据库保存我们发送到MQ消息。如果在Step 3的时候，网络出现故障，Confirm机制没有收到broker的消息确认。
    我们需要设置一个时间临界点，比如说5分钟，检索出消息库中状态为0的消息，通过分布式定时任务，
    比如XXL-Job或者Elastic-Job。但有可能出现消息刚发出去，还没有Confirm成功，定时任务就已经启动了，
    并把发送成功的消息确认为未成功，所以我们需要有一个Step 6,给入库消息一个最大的容忍时间，比如说2分钟到5分钟。
    比如消息入库的时候需要带上时间，我们取出状态为0的消息形成一个集合，然后过滤该集合的时间为2分钟以上的消息进行重新发送。
    由于MQ消息的配置本身有问题的情况下(比如说路由，队列，交换机)，会出现消息永远无法发送成功，这个时候我们需要有一个消息重试的机制，
    比如3次，如果3次都没有发送成功，则更新该消息状态为2，表示失败。
|-如何确保消费者消费了消息？
使用ACK手动确认机制
|-如何避免消息重复投递或重复消费？
使用全局MessageId
Message message = MessageBuilder.withBody(msg.getBytes()).setContentType(MessageProperties.CONTENT_TYPE_JSON)
.setContentEncoding("UTF-8").setMessageId(UUID或订单号).build();
|-如何确保消息不丢失？
持久化
|-RabbitMQ有什么好处？
异步、解耦、流量削峰
|-死信队列条件
消息被拒绝 (basic.reject or basic.nack) 且带 requeue=false不重新入队参数或达到的retry重新入队的上限次数
消息的TTL(Time To Live)-存活时间已经过期
队列长度限制被超越（队列满，queue的"x-max-length"参数）
|-集群模式
  |--普通模式（默认）
     |---比如两个节点A、B，队列存储在A中，消费者从B消费时，A和B会建立临时的消息传输（A->B->消费者）
     |---缺点：当A挂故障后，消费者将消费不了消息。
  |--镜像模式：
     |---把需要的队列做成镜像队列，存在于每个节点
     |---缺点：降低了系统性能、如果镜像队列数量过多，集群内部的网络带宽将会被这种同步通讯大大消耗掉

|-与ActiveMQ的区别
  |--使用Java开发的，使用Erlang开发的
  |--支持JMS，支持AMQP协议
     |---JMS是java API，AMQP是一种链接协议
     |---JMS不能跨语言，AMQP可以
     |---消息模型不同：Peer-2-Peer、Pub/sub；交换机模式


4、zookeeper
|-应用场景
  |--1.注册中心   2.分布式配置中心   3.负载均衡   4.分布式锁  5.发布订阅  6.集群管理
|-四种类型的znode
  |--持久化目录节点、持久化顺序编号目录节点、临时目录节点、临时顺序编号目录节点
|-原理：
  |--文件系统【树状目录结构】
  |--通知机制【监听器】

5、Kafka
|-Topic：每个topic，Kafka集群都会维护一个分区log，Kafka中采用分区的设计有几个目的
  |--1、可以处理更多的消息，不受单台服务器的限制
  |--2、分区可以作为并行处理的单元
  |--Log的分区被分布到集群中的多个服务器上，每个分区有一个leader，零或多个follower，一台服务器可能同时是一个分区的leader，另一个分区的follower
|-生产者：
  |--如何确定消息发送到哪个分区？
     |---首先消息内容包含了key-value键值对
     |---如果Key为null，则会随机分配一个分区；否则，hash(key) % partitions.size
|-消费者：
  |--一个分区只能被同一个消费者组中的一个消费者消费，所以保证了顺序消费
  |--Zookeerper中保存这每个topic下的每个partition在每个group中消费的offset
|-Kafka为什么那么快？
  |--顺序写磁盘，这样比随机写磁盘快
  |--使用了Page Cache，上面使用磁盘性能肯定没有内存快，所以Kafka的数据并不是实时的写入硬盘，它充分利用了现代操作系统分页存储来利用内存提高I/O效率
  |--零拷贝
|-kafka集群中如何确定分区分配到broker上？
  |--topic排序后所在序号对broker 的size取模，结果就是所在broker
  |--例如：topic test有 p0,p1,p2,p3四个分区，有三台broker b1,b2,b3
  |--结果：b1：p0,p3；b2：p1；b3：p2
|-为什么使用Kafka
  |--高吞吐量
  |--持久的消息存储，消息重新消费
  |--需要使用大容量的队列

6、Nginx
|-负载均衡算法
  |--轮询
  |--随机
  |--权重
  |--最小连接
  |--hash
|-Upsync+Consul+Nginx实现动态负载均衡
  |--Upsync：新浪开源的模块，设置定时的去从Consul服务取到最新的数据，然后在本地存储一份配置信息
  |--Upsync安装：从GitHub上下载zip包，解压，然后安装Nginx，在编译的使用使用--add-module=../nginx-upsync-module参数来添加Upsync模块到Nginx
|-LVS+Keepalived+Nginx实现双机主从热备集群，当然，也可以多主多备
  |--LVS：软负载均衡器，抗负载能力比Nginx强，它可以虚拟一个VIP地址，Linux系统自带，直接命令安装，然后配置
  |--Keepalived：在LVS的基础上，实现心跳检测，故障转移，自动重启，通过keepalived.conf配置（注意：每台装有Nginx的服务器都要安装Keepalived）


